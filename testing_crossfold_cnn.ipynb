{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "PENGUJIAN CNN MULTI KERNEL DENGAN SPAM FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 37, 300)      948300      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 37, 256)      230656      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 37, 256)      307456      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 37, 256)      384256      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 18, 256)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 18, 256)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 18, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 18, 768)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 13824)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1769600     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,640,655\n",
      "Trainable params: 3,640,655\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1145 samples, validate on 128 samples\n",
      "Epoch 1/20\n",
      "1145/1145 [==============================] - 13s 11ms/step - loss: 0.6313 - accuracy: 0.6623 - val_loss: 0.6310 - val_accuracy: 0.6745\n",
      "Epoch 2/20\n",
      "1145/1145 [==============================] - 14s 12ms/step - loss: 0.5734 - accuracy: 0.7066 - val_loss: 0.5162 - val_accuracy: 0.7474\n",
      "Epoch 3/20\n",
      "1145/1145 [==============================] - 16s 14ms/step - loss: 0.2900 - accuracy: 0.8908 - val_loss: 0.3512 - val_accuracy: 0.8594\n",
      "Epoch 4/20\n",
      "1145/1145 [==============================] - 12s 10ms/step - loss: 0.0870 - accuracy: 0.9715 - val_loss: 0.4333 - val_accuracy: 0.8724\n",
      "Epoch 5/20\n",
      "1145/1145 [==============================] - 13s 11ms/step - loss: 0.0327 - accuracy: 0.9918 - val_loss: 0.5037 - val_accuracy: 0.8490\n",
      "Epoch 6/20\n",
      "1145/1145 [==============================] - 13s 12ms/step - loss: 0.0200 - accuracy: 0.9936 - val_loss: 0.5974 - val_accuracy: 0.8411\n",
      "Epoch 7/20\n",
      "1145/1145 [==============================] - 12s 10ms/step - loss: 0.0158 - accuracy: 0.9974 - val_loss: 0.5747 - val_accuracy: 0.8490\n",
      "Epoch 8/20\n",
      "1145/1145 [==============================] - 12s 10ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.5000 - val_accuracy: 0.8698\n",
      "Epoch 00008: early stopping\n",
      "319/319 [==============================] - 1s 2ms/step\n",
      "284\n",
      "284\n",
      "284/284 [==============================] - 2s 6ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       87       14      17\n",
      "Negatif        9       81       5\n",
      "Netral        23        1      47\n",
      "{'Positif': 87, 'Negatif': 81, 'Netral': 47}\n",
      "{'Positif': 32, 'Negatif': 15, 'Netral': 22}\n",
      "{'Positif': 31, 'Negatif': 14, 'Netral': 24}\n",
      "{'Positif': 134, 'Netral': 191, 'Negatif': 174}\n",
      "akurasi :  [0.7570422535211268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 37, 300)      955500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 37, 256)      230656      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 37, 256)      307456      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 37, 256)      384256      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 18, 256)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 18, 256)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 18, 256)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 18, 768)      0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 13824)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          1769600     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,647,855\n",
      "Trainable params: 3,647,855\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1145 samples, validate on 128 samples\n",
      "Epoch 1/20\n",
      "1145/1145 [==============================] - 12s 11ms/step - loss: 0.6357 - accuracy: 0.6664 - val_loss: 0.6266 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "1145/1145 [==============================] - 12s 10ms/step - loss: 0.5949 - accuracy: 0.6876 - val_loss: 0.5612 - val_accuracy: 0.7214\n",
      "Epoch 3/20\n",
      "1145/1145 [==============================] - 11s 10ms/step - loss: 0.3459 - accuracy: 0.8588 - val_loss: 0.3582 - val_accuracy: 0.8516\n",
      "Epoch 4/20\n",
      "1145/1145 [==============================] - 12s 10ms/step - loss: 0.1286 - accuracy: 0.9517 - val_loss: 0.4494 - val_accuracy: 0.8464\n",
      "Epoch 5/20\n",
      "1145/1145 [==============================] - 11s 10ms/step - loss: 0.0518 - accuracy: 0.9820 - val_loss: 0.4815 - val_accuracy: 0.8490\n",
      "Epoch 6/20\n",
      "1145/1145 [==============================] - 12s 10ms/step - loss: 0.0268 - accuracy: 0.9921 - val_loss: 0.6372 - val_accuracy: 0.8438\n",
      "Epoch 7/20\n",
      "1145/1145 [==============================] - 14s 12ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.6496 - val_accuracy: 0.8490\n",
      "Epoch 8/20\n",
      "1145/1145 [==============================] - 13s 11ms/step - loss: 0.0130 - accuracy: 0.9971 - val_loss: 0.6498 - val_accuracy: 0.8542\n",
      "Epoch 00008: early stopping\n",
      "319/319 [==============================] - 1s 3ms/step\n",
      "290\n",
      "290\n",
      "290/290 [==============================] - 2s 7ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       92       17       9\n",
      "Negatif        9       77       9\n",
      "Netral        21       12      44\n",
      "{'Positif': 92, 'Negatif': 77, 'Netral': 44}\n",
      "{'Positif': 30, 'Negatif': 29, 'Netral': 18}\n",
      "{'Positif': 26, 'Negatif': 18, 'Netral': 33}\n",
      "{'Positif': 142, 'Negatif': 166, 'Netral': 195}\n",
      "akurasi :  [0.7570422535211268, 0.7344827586206897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 37, 300)      960000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 37, 256)      230656      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 37, 256)      307456      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 37, 256)      384256      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 18, 256)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 18, 256)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 18, 256)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 18, 768)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 13824)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          1769600     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            387         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,652,355\n",
      "Trainable params: 3,652,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1146 samples, validate on 128 samples\n",
      "Epoch 1/20\n",
      "1146/1146 [==============================] - 14s 12ms/step - loss: 0.6306 - accuracy: 0.6658 - val_loss: 0.6371 - val_accuracy: 0.6641\n",
      "Epoch 2/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.5779 - accuracy: 0.7054 - val_loss: 0.5452 - val_accuracy: 0.7161\n",
      "Epoch 3/20\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 0.3078 - accuracy: 0.8700 - val_loss: 0.3310 - val_accuracy: 0.8542\n",
      "Epoch 4/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.0960 - accuracy: 0.9645 - val_loss: 0.5024 - val_accuracy: 0.8464\n",
      "Epoch 5/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.0321 - accuracy: 0.9919 - val_loss: 0.5805 - val_accuracy: 0.8307\n",
      "Epoch 6/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.6329 - val_accuracy: 0.8333\n",
      "Epoch 7/20\n",
      "1146/1146 [==============================] - 11s 9ms/step - loss: 0.0241 - accuracy: 0.9948 - val_loss: 0.6629 - val_accuracy: 0.8307\n",
      "Epoch 8/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.0117 - accuracy: 0.9980 - val_loss: 0.6644 - val_accuracy: 0.8438\n",
      "Epoch 00008: early stopping\n",
      "318/318 [==============================] - 1s 2ms/step\n",
      "291\n",
      "291\n",
      "291/291 [==============================] - 2s 6ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       83        8      27\n",
      "Negatif       10       80       6\n",
      "Netral        20        6      51\n",
      "{'Positif': 83, 'Negatif': 80, 'Netral': 51}\n",
      "{'Positif': 30, 'Negatif': 14, 'Netral': 33}\n",
      "{'Positif': 35, 'Negatif': 16, 'Netral': 26}\n",
      "{'Positif': 143, 'Negatif': 181, 'Netral': 181}\n",
      "akurasi :  [0.7570422535211268, 0.7344827586206897, 0.7353951890034365]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 36, 300)      959100      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 36, 256)      230656      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 36, 256)      307456      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 36, 256)      384256      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 18, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 18, 256)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 18, 256)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 18, 768)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "                                                                 max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 13824)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          1769600     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            387         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,651,455\n",
      "Trainable params: 3,651,455\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1146 samples, validate on 128 samples\n",
      "Epoch 1/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.6286 - accuracy: 0.6670 - val_loss: 0.6207 - val_accuracy: 0.6719\n",
      "Epoch 2/20\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 0.5413 - accuracy: 0.7341 - val_loss: 0.5066 - val_accuracy: 0.7526\n",
      "Epoch 3/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.3007 - accuracy: 0.8752 - val_loss: 0.4416 - val_accuracy: 0.8099\n",
      "Epoch 4/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.1080 - accuracy: 0.9648 - val_loss: 0.5550 - val_accuracy: 0.8385\n",
      "Epoch 5/20\n",
      "1146/1146 [==============================] - 11s 10ms/step - loss: 0.0394 - accuracy: 0.9898 - val_loss: 0.6102 - val_accuracy: 0.8359\n",
      "Epoch 6/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.0303 - accuracy: 0.9892 - val_loss: 0.6383 - val_accuracy: 0.8255\n",
      "Epoch 7/20\n",
      "1146/1146 [==============================] - 13s 12ms/step - loss: 0.0208 - accuracy: 0.9959 - val_loss: 0.5721 - val_accuracy: 0.8438\n",
      "Epoch 8/20\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.6119 - val_accuracy: 0.8437\n",
      "Epoch 00008: early stopping\n",
      "318/318 [==============================] - 1s 2ms/step\n",
      "277\n",
      "277\n",
      "277/277 [==============================] - 2s 7ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       81        5      30\n",
      "Negatif       17       77       3\n",
      "Netral        13        5      46\n",
      "{'Positif': 81, 'Negatif': 77, 'Netral': 46}\n",
      "{'Positif': 30, 'Negatif': 10, 'Netral': 33}\n",
      "{'Positif': 35, 'Negatif': 20, 'Netral': 18}\n",
      "{'Positif': 131, 'Negatif': 170, 'Netral': 180}\n",
      "akurasi :  [0.7570422535211268, 0.7344827586206897, 0.7353951890034365, 0.7364620938628159]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:134: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:135: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:151: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 37, 300)      930900      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 37, 256)      230656      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 37, 256)      307456      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 37, 256)      384256      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 18, 256)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 18, 256)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 18, 256)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 18, 768)      0           max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "                                                                 max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 13824)        0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          1769600     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            387         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,623,255\n",
      "Trainable params: 3,623,255\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1146 samples, validate on 128 samples\n",
      "Epoch 1/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.6289 - accuracy: 0.6661 - val_loss: 0.6543 - val_accuracy: 0.6589\n",
      "Epoch 2/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.5634 - accuracy: 0.7187 - val_loss: 0.5935 - val_accuracy: 0.6979\n",
      "Epoch 3/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.3558 - accuracy: 0.8365 - val_loss: 0.5790 - val_accuracy: 0.7656\n",
      "Epoch 4/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.1883 - accuracy: 0.9220 - val_loss: 0.7848 - val_accuracy: 0.7812\n",
      "Epoch 5/20\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 0.1069 - accuracy: 0.9607 - val_loss: 0.6581 - val_accuracy: 0.8490\n",
      "Epoch 6/20\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 0.0449 - accuracy: 0.9840 - val_loss: 0.8748 - val_accuracy: 0.7943\n",
      "Epoch 7/20\n",
      "1146/1146 [==============================] - 12s 10ms/step - loss: 0.0477 - accuracy: 0.9872 - val_loss: 0.9807 - val_accuracy: 0.8125\n",
      "Epoch 8/20\n",
      "1146/1146 [==============================] - 12s 11ms/step - loss: 0.0239 - accuracy: 0.9936 - val_loss: 0.8568 - val_accuracy: 0.8099\n",
      "Epoch 00008: early stopping\n",
      "318/318 [==============================] - 1s 2ms/step\n",
      "282\n",
      "282\n",
      "282/282 [==============================] - 2s 8ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       90        7      19\n",
      "Negatif        8       79       8\n",
      "Netral        24        4      43\n",
      "{'Positif': 90, 'Negatif': 79, 'Netral': 43}\n",
      "{'Positif': 32, 'Negatif': 11, 'Netral': 27}\n",
      "{'Positif': 26, 'Negatif': 16, 'Netral': 28}\n",
      "{'Positif': 134, 'Negatif': 176, 'Netral': 184}\n",
      "akurasi :  [0.7570422535211268, 0.7344827586206897, 0.7353951890034365, 0.7364620938628159, 0.75177304964539]\n",
      "{'acc': [0.7570422535211268, 0.7344827586206897, 0.7353951890034365, 0.7364620938628159, 0.75177304964539], 'presisi': [0.752000619088215, 0.7300636247833996, 0.7309066537554582, 0.7323552273355852, 0.7432561366987597], 'recall': [0.7506305151755015, 0.7205386347223991, 0.7330202753931568, 0.7369467650195521, 0.7376916063836133]}\n",
      "74.30310689306918\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import nltk\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from statistics import mean \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "data = pd.read_csv('D:/Tugas/Skripsi/tambah data neg hasil.csv' , encoding='unicode_escape',header = None)\n",
    "\n",
    "data.columns = ['Id','Text_Final','Kategori']\n",
    "\n",
    "data = data.reset_index()\n",
    "\n",
    "data['tokens'] = [word_tokenize(sen) for sen in data['Text_Final']]\n",
    "\n",
    "def removeDoublecharWordList(word_list):\n",
    "    return [removeDoublechar(word) for word in word_list]\n",
    "\n",
    "def removeDoublechar(word):\n",
    "    newWord = list(word)\n",
    "    charsList = enumerate(list(word))\n",
    "    for i,c in charsList:\n",
    "        if i > len(newWord) -2:\n",
    "            break\n",
    "        if newWord[i+1] == c:\n",
    "            newWord[i+1] = ''\n",
    "    return ''.join(newWord)\n",
    "\n",
    "dataToken = data.tokens.apply(removeDoublecharWordList).reset_index()['tokens']\n",
    "data['Text_Final'] = [' '.join(sen) for sen in dataToken]\n",
    "xc=data['Text_Final']\n",
    "yc=data['Kategori']\n",
    "# print(type(xc))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle = False)\n",
    "\n",
    "acc=[]\n",
    "presisi=[]\n",
    "recal=[]\n",
    "fold = 1\n",
    "kelas0_tpr = []\n",
    "kelas0_auc = []\n",
    "kelas1_tpr = []\n",
    "kelas1_auc = []\n",
    "kelas2_tpr = []\n",
    "kelas2_auc = []\n",
    "kelas3_tpr = []\n",
    "kelas3_auc = []\n",
    "micro_tpr=[]\n",
    "micro_auc=[]\n",
    "all_fpr0=[]\n",
    "all_tpr0=[]\n",
    "x = data['Text_Final']\n",
    "y = data['Kategori']\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "json_file_spam = open('D:\\Tugas\\Skripsi\\model\\model_spam6_kim.json', 'r')\n",
    "model_json_spam = json_file_spam.read()\n",
    "json_file_spam.close()\n",
    "model_spam = model_from_json(model_json_spam)\n",
    "model_spam.load_weights('D:\\Tugas\\Skripsi\\model\\model_spam6_kim.h5')\n",
    "\n",
    "with open('D:/Tugas/Skripsi/model/tokenizer_model_spam6_kim.pickle', 'rb') as handle:\n",
    "    tokenizer_spam = pickle.load(handle)\n",
    "    \n",
    "\n",
    "for train, test in kfold.split(x, y): \n",
    "    labelss= pd.DataFrame(columns=['Negatif','Positif','Netral'])\n",
    "    neg = []\n",
    "    pos = []\n",
    "    net = []\n",
    "\n",
    "    for l in y[train]:\n",
    "        if l == 'Negatif':\n",
    "            neg.append(0)\n",
    "            pos.append(1)\n",
    "            net.append(0)\n",
    "        if l == 'Positif':\n",
    "            neg.append(1)\n",
    "            pos.append(0)\n",
    "            net.append(0)\n",
    "        if l == 'Netral':\n",
    "            neg.append(0)\n",
    "            pos.append(0)\n",
    "            net.append(1)\n",
    "\n",
    "    labelss['Positif']= pos\n",
    "    labelss['Negatif']= neg\n",
    "    labelss['Netral']= net\n",
    "    \n",
    "    x[train]=x[train].astype(str)\n",
    "    x[test]=x[test].astype(str)\n",
    "    \n",
    "    tok = [word_tokenize(sen) for sen in x[train]]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x[train].tolist())\n",
    "    train_word_index = tokenizer.word_index\n",
    "    training_sequences = tokenizer.texts_to_sequences(x[train].tolist())\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = max(len(x) for x in training_sequences)\n",
    "    EMBEDDING_DIM = 300\n",
    "    \n",
    "    model_word = Word2Vec(dataToken, size=300, min_count=1)\n",
    "    embeddings_index = {}\n",
    "    for i in range(len(model_word.wv.vocab)):\n",
    "        word = list(model_word.wv.vocab)[i]\n",
    "        coefs = model_word[word]\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    \n",
    "    train_lstm_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    y_train = labelss.values\n",
    "    x_train = train_lstm_data\n",
    "    \n",
    "#     print(len(x_train))\n",
    "\n",
    "    es_callback =EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    \n",
    "     # Kim Yoon CNN\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Concatenate\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        xx = Conv1D(256, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "        xx = MaxPooling1D()(xx)\n",
    "        convs.append(xx)\n",
    "\n",
    "    xx = Concatenate(axis=-1)(convs)\n",
    "    xx = Flatten()(xx)\n",
    "    xx = Dense(128, activation='relu')(xx)\n",
    "    xx= Dropout(0.3)(xx)\n",
    "    output = Dense(3, activation='softmax')(xx)\n",
    "\n",
    "    model = Model(sequence_input, output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=20,\n",
    "                        verbose=True,\n",
    "                        validation_split=(0.1),\n",
    "                        batch_size=64,shuffle = True,\n",
    "                        callbacks=[es_callback])\n",
    "        \n",
    "\n",
    "    test_sequences = tokenizer_spam.texts_to_sequences(x[test].tolist())\n",
    "    test_data = pad_sequences(test_sequences, maxlen=36)\n",
    "\n",
    "    xs = []\n",
    "    lab_sen = []\n",
    "    ul_sen = []\n",
    "    prediksi_spam=[]\n",
    "    predik_spam=[]\n",
    "    ys = []\n",
    "    yt = []\n",
    "    ulasan_spam = []\n",
    "    result_spam = model_spam.predict(test_data, batch_size=1, verbose=1)\n",
    "    class_spam = ['SPAM','BUKAN SPAM']\n",
    "    \n",
    "    y_test= y[test].values\n",
    "    for i in range(0, len(y_test)):\n",
    "        yt.append(y_test[i])\n",
    "\n",
    "    ulasants = x[test].values\n",
    "    for i in range(0, len(ulasants)):\n",
    "        ys.append(ulasants[i])\n",
    "\n",
    "    for i in range(len(result_spam)):\n",
    "        predik_spam.append(class_spam[result_spam[i].argmax()])\n",
    "        if predik_spam[i] == 'BUKAN SPAM':\n",
    "            lab_sen.append(yt[i])\n",
    "            ul_sen.append(ys[i])\n",
    "        else:\n",
    "            prediksi_spam.append('Netral')\n",
    "            ulasan_spam.append(ys[i])\n",
    "\n",
    "    dat_ul = []\n",
    "    y_tes = lab_sen\n",
    "    dat_ul = ul_sen\n",
    "    \n",
    "    print(len(y_tes))\n",
    "    print(len(dat_ul))\n",
    "\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences(dat_ul)\n",
    "    test_lstm_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "    predictions = model.predict(test_lstm_data, batch_size=1, verbose=1)\n",
    "    labels= ['Positif','Negatif','Netral']\n",
    "\n",
    "    prediction_labels=[]\n",
    "    for p in predictions:\n",
    "        prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "#     xx=sum(y[test]==prediction_labels)/len(prediction_labels)\n",
    "\n",
    "\n",
    "    y_pred=[]\n",
    "    for p in predictions:\n",
    "        y_pred.append(labels[np.argmax(p)])\n",
    "    cf = []\n",
    "    cf_kategori = pd.DataFrame(\n",
    "    data=confusion_matrix(y_tes, y_pred, labels=labels),\n",
    "    columns=labels,\n",
    "    index=labels\n",
    "    )\n",
    "    cf.append(cf_kategori)\n",
    "    print(cf_kategori)\n",
    "\n",
    "    tps_kategori = {}\n",
    "    fps_kategori  = {}\n",
    "    fns_kategori  = {}\n",
    "    tns_kategori  = {}\n",
    "    for label in labels:\n",
    "        tps_kategori[label] = cf_kategori.loc[label, label]\n",
    "        fps_kategori[label] = cf_kategori[label].sum() - tps_kategori[label]\n",
    "        fns_kategori[label] = cf_kategori.loc[label].sum() - tps_kategori[label]\n",
    "\n",
    "    for label in set(y_tes):\n",
    "        tns_kategori[label] = len(y_tes) - (tps_kategori[label] + fps_kategori[label] + fns_kategori[label])\n",
    "\n",
    "    print(tps_kategori)\n",
    "    print(fps_kategori)\n",
    "    print(fns_kategori)\n",
    "    print(tns_kategori)\n",
    "    accuracyKategori=sum(tps_kategori.values())/len(y_tes)\n",
    "    acc.append(accuracyKategori)\n",
    "\n",
    "\n",
    "    tpfp_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fps_kategori.values()))]\n",
    "    precision=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfp_kategori)]\n",
    "    precisionKategori=sum(precision)/3\n",
    "    presisi.append(precisionKategori)\n",
    "\n",
    "    tpfn_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fns_kategori.values()))]\n",
    "    recall=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfn_kategori)]\n",
    "    recallKategori=sum(recall)/3\n",
    "    recal.append(recallKategori)\n",
    "    print('akurasi : ',acc)\n",
    "    \n",
    "#     model_json = model.to_json()\n",
    "#     with open('D:\\Tugas\\Skripsi\\model\\model_cnndenganspam256_kim_'+str(fold)+'.json','w') as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     model.save_weights('D:\\Tugas\\Skripsi\\model\\model_cnndenganspam256_kim_'+str(fold)+'.h5')\n",
    "#     print(\"Model saved to disk\")\n",
    "#     with open('D:/Tugas/Skripsi/model/tokenizer_model_model_cnndenganspam256_kim_'+str(fold)+'.pickle', 'wb') as handle:\n",
    "#         pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     fold = fold+1\n",
    "hasil={'acc':acc,'presisi':presisi,'recall':recal}\n",
    "print(hasil)\n",
    "\n",
    "# with open ('D:/Tugas/Skripsi/testing/1/hasil_kategori_kfold10_40.json','w') as json_file:\n",
    "#     json.dump(hasil,json_file)\n",
    "\n",
    "print(mean(acc)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PENGUJIAN CNN MULTI KERNEL TANPA SPAM FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 37, 100)      318800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 37, 128)      38528       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 37, 128)      51328       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 37, 128)      64128       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 18, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 18, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 18, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 18, 384)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6912)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          884864      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,358,035\n",
      "Trainable params: 1,358,035\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1018 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1018/1018 [==============================] - 5s 5ms/step - loss: 0.6310 - accuracy: 0.6598 - val_loss: 0.6329 - val_accuracy: 0.6732\n",
      "Epoch 2/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.5565 - accuracy: 0.7171 - val_loss: 0.4609 - val_accuracy: 0.8118\n",
      "Epoch 3/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.1999 - accuracy: 0.9250 - val_loss: 0.4665 - val_accuracy: 0.8052\n",
      "Epoch 4/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0655 - accuracy: 0.9840 - val_loss: 0.5943 - val_accuracy: 0.8157\n",
      "Epoch 5/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0282 - accuracy: 0.9905 - val_loss: 0.6720 - val_accuracy: 0.8131\n",
      "Epoch 6/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.8342 - val_accuracy: 0.8092\n",
      "Epoch 7/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.8460 - val_accuracy: 0.8000\n",
      "Epoch 00007: early stopping\n",
      "319/319 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       79       16       3\n",
      "Positif        6      101      12\n",
      "Netral         7       36      59\n",
      "{'Negatif': 79, 'Positif': 101, 'Netral': 59}\n",
      "{'Negatif': 13, 'Positif': 52, 'Netral': 15}\n",
      "{'Negatif': 19, 'Positif': 18, 'Netral': 43}\n",
      "{'Positif': 148, 'Negatif': 208, 'Netral': 202}\n",
      "akurasi :  [0.7492163009404389]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 36, 100)      320400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 36, 128)      38528       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 36, 128)      51328       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 36, 128)      64128       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 18, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 18, 128)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 18, 128)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 18, 384)      0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 6912)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          884864      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,359,635\n",
      "Trainable params: 1,359,635\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1018 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1018/1018 [==============================] - 5s 5ms/step - loss: 0.6352 - accuracy: 0.6640 - val_loss: 0.6243 - val_accuracy: 0.6667\n",
      "Epoch 2/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.5567 - accuracy: 0.7056 - val_loss: 0.4156 - val_accuracy: 0.8065\n",
      "Epoch 3/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.2234 - accuracy: 0.9096 - val_loss: 0.4021 - val_accuracy: 0.8183\n",
      "Epoch 4/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0631 - accuracy: 0.9823 - val_loss: 0.5221 - val_accuracy: 0.8379\n",
      "Epoch 5/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0305 - accuracy: 0.9931 - val_loss: 0.5071 - val_accuracy: 0.8340\n",
      "Epoch 6/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0261 - accuracy: 0.9931 - val_loss: 0.6170 - val_accuracy: 0.8458\n",
      "Epoch 7/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.6296 - val_accuracy: 0.8301\n",
      "Epoch 8/30\n",
      "1018/1018 [==============================] - 4s 4ms/step - loss: 0.0217 - accuracy: 0.9895 - val_loss: 0.5961 - val_accuracy: 0.8275\n",
      "Epoch 00008: early stopping\n",
      "319/319 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       85        7       6\n",
      "Positif       16       78      25\n",
      "Netral         7       21      74\n",
      "{'Negatif': 85, 'Positif': 78, 'Netral': 74}\n",
      "{'Negatif': 23, 'Positif': 28, 'Netral': 31}\n",
      "{'Negatif': 13, 'Positif': 41, 'Netral': 28}\n",
      "{'Positif': 172, 'Netral': 186, 'Negatif': 198}\n",
      "akurasi :  [0.7492163009404389, 0.7429467084639498]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 37, 100)      312900      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 37, 128)      38528       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 37, 128)      51328       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 37, 128)      64128       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 18, 128)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 18, 128)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 18, 128)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 18, 384)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 6912)         0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          884864      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            387         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,352,135\n",
      "Trainable params: 1,352,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1019/1019 [==============================] - 5s 5ms/step - loss: 0.6317 - accuracy: 0.6605 - val_loss: 0.6296 - val_accuracy: 0.6680\n",
      "Epoch 2/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.5557 - accuracy: 0.7085 - val_loss: 0.4988 - val_accuracy: 0.7386\n",
      "Epoch 3/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.2332 - accuracy: 0.9009 - val_loss: 0.4658 - val_accuracy: 0.8157\n",
      "Epoch 4/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0770 - accuracy: 0.9745 - val_loss: 0.5466 - val_accuracy: 0.8144\n",
      "Epoch 5/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0363 - accuracy: 0.9892 - val_loss: 0.6743 - val_accuracy: 0.8131\n",
      "Epoch 6/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0175 - accuracy: 0.9974 - val_loss: 0.7175 - val_accuracy: 0.8209\n",
      "Epoch 7/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0107 - accuracy: 0.9980 - val_loss: 0.7383 - val_accuracy: 0.8183\n",
      "Epoch 8/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0084 - accuracy: 0.9987 - val_loss: 0.7917 - val_accuracy: 0.8092\n",
      "Epoch 00008: early stopping\n",
      "318/318 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       73       20       4\n",
      "Positif       14       88      17\n",
      "Netral         7       26      69\n",
      "{'Negatif': 73, 'Positif': 88, 'Netral': 69}\n",
      "{'Negatif': 21, 'Positif': 46, 'Netral': 21}\n",
      "{'Negatif': 24, 'Positif': 31, 'Netral': 33}\n",
      "{'Positif': 153, 'Netral': 195, 'Negatif': 200}\n",
      "akurasi :  [0.7492163009404389, 0.7429467084639498, 0.7232704402515723]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 37, 100)      315700      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 37, 128)      38528       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 37, 128)      51328       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 37, 128)      64128       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 18, 128)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 18, 128)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 18, 128)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 18, 384)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "                                                                 max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 6912)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          884864      flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            387         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,354,935\n",
      "Trainable params: 1,354,935\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1019/1019 [==============================] - 5s 5ms/step - loss: 0.6309 - accuracy: 0.6640 - val_loss: 0.6402 - val_accuracy: 0.6654\n",
      "Epoch 2/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.5528 - accuracy: 0.7161 - val_loss: 0.4796 - val_accuracy: 0.7582\n",
      "Epoch 3/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.2206 - accuracy: 0.9123 - val_loss: 0.4185 - val_accuracy: 0.8392\n",
      "Epoch 4/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0533 - accuracy: 0.9830 - val_loss: 0.5366 - val_accuracy: 0.8301\n",
      "Epoch 5/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.5726 - val_accuracy: 0.8314\n",
      "Epoch 6/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 0.6504 - val_accuracy: 0.8222\n",
      "Epoch 7/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 0.7448 - val_accuracy: 0.8170\n",
      "Epoch 8/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.7403 - val_accuracy: 0.8235\n",
      "Epoch 00008: early stopping\n",
      "318/318 [==============================] - 1s 4ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       71       20       6\n",
      "Positif       16       87      16\n",
      "Netral         9       23      70\n",
      "{'Negatif': 71, 'Positif': 87, 'Netral': 70}\n",
      "{'Negatif': 25, 'Positif': 43, 'Netral': 22}\n",
      "{'Negatif': 26, 'Positif': 32, 'Netral': 32}\n",
      "{'Positif': 156, 'Netral': 194, 'Negatif': 196}\n",
      "akurasi :  [0.7492163009404389, 0.7429467084639498, 0.7232704402515723, 0.7169811320754716]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:140: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 37, 100)      314600      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 37, 128)      38528       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 37, 128)      51328       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 37, 128)      64128       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 18, 128)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 18, 128)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 18, 128)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 18, 384)      0           max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "                                                                 max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 6912)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          884864      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            387         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,353,835\n",
      "Trainable params: 1,353,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1019/1019 [==============================] - 5s 5ms/step - loss: 0.6323 - accuracy: 0.6595 - val_loss: 0.6337 - val_accuracy: 0.6719\n",
      "Epoch 2/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.5658 - accuracy: 0.7033 - val_loss: 0.4552 - val_accuracy: 0.7961\n",
      "Epoch 3/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.2624 - accuracy: 0.8937 - val_loss: 0.4155 - val_accuracy: 0.8275\n",
      "Epoch 4/30\n",
      "1019/1019 [==============================] - 4s 4ms/step - loss: 0.0685 - accuracy: 0.9797 - val_loss: 0.5373 - val_accuracy: 0.8340\n",
      "Epoch 5/30\n",
      "1019/1019 [==============================] - 5s 4ms/step - loss: 0.0308 - accuracy: 0.9892 - val_loss: 0.6242 - val_accuracy: 0.8353\n",
      "Epoch 6/30\n",
      "1019/1019 [==============================] - 5s 5ms/step - loss: 0.0212 - accuracy: 0.9935 - val_loss: 0.7110 - val_accuracy: 0.8222\n",
      "Epoch 7/30\n",
      "1019/1019 [==============================] - 5s 4ms/step - loss: 0.0152 - accuracy: 0.9964 - val_loss: 0.6449 - val_accuracy: 0.8418\n",
      "Epoch 8/30\n",
      "1019/1019 [==============================] - 5s 4ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.6638 - val_accuracy: 0.8431\n",
      "Epoch 00008: early stopping\n",
      "318/318 [==============================] - 1s 4ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       78       11       8\n",
      "Positif       11       83      25\n",
      "Netral         8       15      79\n",
      "{'Negatif': 78, 'Positif': 83, 'Netral': 79}\n",
      "{'Negatif': 19, 'Positif': 26, 'Netral': 33}\n",
      "{'Negatif': 19, 'Positif': 36, 'Netral': 23}\n",
      "{'Positif': 173, 'Netral': 183, 'Negatif': 202}\n",
      "akurasi :  [0.7492163009404389, 0.7429467084639498, 0.7232704402515723, 0.7169811320754716, 0.7547169811320755]\n",
      "{'acc': [0.7492163009404389, 0.7429467084639498, 0.7232704402515723, 0.7169811320754716, 0.7547169811320755], 'presisi': [0.7720412228084862, 0.7425493328009051, 0.7333262764193219, 0.7232278892604981, 0.7569829147018686], 'recall': [0.7444311057756435, 0.7494331065759637, 0.722847902047417, 0.7164419032217698, 0.7587041689528045]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import nltk\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "data = pd.read_csv('D:/Tugas/Skripsi/tambah data neg hasil.csv' , encoding='unicode_escape',header = None)\n",
    "\n",
    "data.columns = ['Id','Text_Final','Kategori']\n",
    "\n",
    "data = data.reset_index()\n",
    "\n",
    "data['tokens'] = [word_tokenize(sen) for sen in data['Text_Final']]\n",
    "\n",
    "def removeDoublecharWordList(word_list):\n",
    "    return [removeDoublechar(word) for word in word_list]\n",
    "\n",
    "def removeDoublechar(word):\n",
    "    newWord = list(word)\n",
    "    charsList = enumerate(list(word))\n",
    "    for i,c in charsList:\n",
    "        if i > len(newWord) -2:\n",
    "            break\n",
    "        if newWord[i+1] == c:\n",
    "            newWord[i+1] = ''\n",
    "    return ''.join(newWord)\n",
    "\n",
    "dataToken = data.tokens.apply(removeDoublecharWordList).reset_index()['tokens']\n",
    "data['Text_Final'] = [' '.join(sen) for sen in dataToken]\n",
    "x=data['Text_Final']\n",
    "y=data['Kategori']\n",
    "# print(type(xc))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle = False)\n",
    "\n",
    "acc=[]\n",
    "presisi=[]\n",
    "recal=[]\n",
    "fold = 1\n",
    "kelas0_tpr = []\n",
    "kelas0_auc = []\n",
    "kelas1_tpr = []\n",
    "kelas1_auc = []\n",
    "kelas2_tpr = []\n",
    "kelas2_auc = []\n",
    "kelas3_tpr = []\n",
    "kelas3_auc = []\n",
    "micro_tpr=[]\n",
    "micro_auc=[]\n",
    "all_fpr0=[]\n",
    "all_tpr0=[]\n",
    "# x = data['Text_Final']\n",
    "# y = data['Kategori']\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "for train, test in kfold.split(x, y): \n",
    "    labelss= pd.DataFrame(columns=['Negatif','Positif','Netral'])\n",
    "    neg = []\n",
    "    pos = []\n",
    "    net = []\n",
    "#     print(test)\n",
    "\n",
    "    for l in y[train]:\n",
    "        if l == 'Negatif':\n",
    "            neg.append(1)\n",
    "            pos.append(0)\n",
    "            net.append(0)\n",
    "        if l == 'Positif':\n",
    "            neg.append(0)\n",
    "            pos.append(1)\n",
    "            net.append(0)\n",
    "        if l == 'Netral':\n",
    "            neg.append(0)\n",
    "            pos.append(0)\n",
    "            net.append(1)\n",
    "\n",
    "    labelss['Negatif']= neg\n",
    "    labelss['Positif']= pos\n",
    "    labelss['Netral']= net\n",
    "    \n",
    "    x[train]=x[train].astype(str)\n",
    "    x[test]=x[test].astype(str)\n",
    "    \n",
    "    tok = [word_tokenize(sen) for sen in x[train]]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x[train].tolist())\n",
    "    train_word_index = tokenizer.word_index\n",
    "    training_sequences = tokenizer.texts_to_sequences(x[train].tolist())\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = max(len(x) for x in training_sequences)\n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model = Word2Vec(dataToken, size=100, min_count=1)\n",
    "\n",
    "    embeddings_index = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        word = list(model.wv.vocab)[i]\n",
    "        coefs = model[word]\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    \n",
    "    train_lstm_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    y_train = labelss.values\n",
    "    x_train = train_lstm_data\n",
    "\n",
    "    es_callback =EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    \n",
    "     # Kim Yoon CNN\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Concatenate\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM,  weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        xx = Conv1D(128, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "        xx = MaxPooling1D()(xx)\n",
    "        convs.append(xx)\n",
    "\n",
    "    xx = Concatenate(axis=-1)(convs)\n",
    "    xx = Flatten()(xx)\n",
    "    xx = Dense(128, activation='relu')(xx)\n",
    "    xx= Dropout(0.3)(xx)\n",
    "    output = Dense(3, activation='softmax')(xx)\n",
    "\n",
    "    model = Model(sequence_input, output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=30,\n",
    "                        verbose=True,\n",
    "                        validation_split=(0.2),\n",
    "                        batch_size=16,shuffle = True,\n",
    "                        callbacks=[es_callback])\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences(x[test].tolist())\n",
    "    test_lstm_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    \n",
    "    predictions = model.predict(test_lstm_data, batch_size=1, verbose=1)\n",
    "    labels= ['Negatif','Positif','Netral']\n",
    "\n",
    "    prediction_labels=[]\n",
    "    for p in predictions:\n",
    "        prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "\n",
    "    y_test= y[test].values\n",
    "    ulsan_fold = []\n",
    "    label_fold = []\n",
    "    y_tes=[]\n",
    "    for i in range(0, len(y_test)):\n",
    "        y_tes.append(y_test[i])\n",
    "\n",
    "    y_pred=[]\n",
    "    for p in predictions:\n",
    "        y_pred.append(labels[np.argmax(p)])\n",
    "    cf = []\n",
    "    cf_kategori = pd.DataFrame(\n",
    "    data=confusion_matrix(y_tes, y_pred, labels=labels),\n",
    "    columns=labels,\n",
    "    index=labels\n",
    "    )\n",
    "    cf.append(cf_kategori)\n",
    "    print(cf_kategori)\n",
    "\n",
    "    tps_kategori = {}\n",
    "    fps_kategori  = {}\n",
    "    fns_kategori  = {}\n",
    "    tns_kategori  = {}\n",
    "    for label in labels:\n",
    "        tps_kategori[label] = cf_kategori.loc[label, label]\n",
    "        fps_kategori[label] = cf_kategori[label].sum() - tps_kategori[label]\n",
    "        fns_kategori[label] = cf_kategori.loc[label].sum() - tps_kategori[label]\n",
    "\n",
    "    for label in set(y_tes):\n",
    "        tns_kategori[label] = len(y_tes) - (tps_kategori[label] + fps_kategori[label] + fns_kategori[label])\n",
    "\n",
    "    print(tps_kategori)\n",
    "    print(fps_kategori)\n",
    "    print(fns_kategori)\n",
    "    print(tns_kategori)\n",
    "    accuracyKategori=sum(tps_kategori.values())/len(y_tes)\n",
    "    acc.append(accuracyKategori)\n",
    "\n",
    "\n",
    "    tpfp_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fps_kategori.values()))]\n",
    "    precision=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfp_kategori)]\n",
    "    precisionKategori=sum(precision)/3\n",
    "    presisi.append(precisionKategori)\n",
    "\n",
    "    tpfn_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fns_kategori.values()))]\n",
    "    recall=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfn_kategori)]\n",
    "    recallKategori=sum(recall)/3\n",
    "    recal.append(recallKategori)\n",
    "    print('akurasi : ',acc)\n",
    "\n",
    "hasil={'acc':acc,'presisi':presisi,'recall':recal}\n",
    "print(hasil)\n",
    "# with open ('D:/Tugas/Skripsi/testing/1/hasil_tanpaspam_kategori_kfold5.json','w') as json_file:\n",
    "#     json.dump(hasil,json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean(acc)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.12077837582066\n",
      "74.87273515900712\n",
      "73.74263125727016\n",
      "65.59242437389005\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "ds = [0.7523510971786834, 0.7711598746081505, 0.7578616352201258, 0.7389937106918238, 0.7232704402515723]\n",
    "ds2 = [0.7210031347962382, 0.7586206896551724, 0.7358490566037735, 0.7452830188679245, 0.7452830188679245]\n",
    "dd =  [0.7492163009404389, 0.7429467084639498, 0.7232704402515723, 0.7169811320754716, 0.7547169811320755]\n",
    "df = [0.5928192574459404, 0.6715606952740005, 0.6818086883876356, 0.6272201853280222, 0.7062123922589039]\n",
    "print(mean(ds2)*100)\n",
    "print(mean(ds)*100)\n",
    "print(mean(dd)*100)\n",
    "print(mean(df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PENGUJIAN CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 37)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 37, 300)           948300    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 37, 256)           384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,922,895\n",
      "Trainable params: 1,922,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1018 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.6073 - accuracy: 0.6853 - val_loss: 0.5492 - val_accuracy: 0.7320\n",
      "Epoch 2/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.3285 - accuracy: 0.8635 - val_loss: 0.4895 - val_accuracy: 0.8026\n",
      "Epoch 3/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.1030 - accuracy: 0.9623 - val_loss: 0.6212 - val_accuracy: 0.8144\n",
      "Epoch 4/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0333 - accuracy: 0.9908 - val_loss: 0.9087 - val_accuracy: 0.7908\n",
      "Epoch 5/30\n",
      "1018/1018 [==============================] - 8s 8ms/step - loss: 0.0291 - accuracy: 0.9944 - val_loss: 0.7505 - val_accuracy: 0.8105\n",
      "Epoch 6/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0188 - accuracy: 0.9921 - val_loss: 0.8480 - val_accuracy: 0.8092\n",
      "Epoch 7/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 1.0261 - val_accuracy: 0.8118\n",
      "Epoch 00007: early stopping\n",
      "319/319 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       69       21       8\n",
      "Positif       12       97      10\n",
      "Netral         2       36      64\n",
      "{'Negatif': 69, 'Positif': 97, 'Netral': 64}\n",
      "{'Negatif': 14, 'Positif': 57, 'Netral': 18}\n",
      "{'Negatif': 29, 'Positif': 22, 'Netral': 38}\n",
      "{'Positif': 143, 'Netral': 199, 'Negatif': 207}\n",
      "akurasi :  [0.7210031347962382]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 37)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 37, 300)           955500    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 37, 256)           384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 18, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,930,095\n",
      "Trainable params: 1,930,095\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1018 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1018/1018 [==============================] - 9s 9ms/step - loss: 0.6064 - accuracy: 0.6778 - val_loss: 0.4953 - val_accuracy: 0.7673\n",
      "Epoch 2/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.2972 - accuracy: 0.8795 - val_loss: 0.3807 - val_accuracy: 0.8327\n",
      "Epoch 3/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0883 - accuracy: 0.9728 - val_loss: 0.5033 - val_accuracy: 0.8340\n",
      "Epoch 4/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0334 - accuracy: 0.9908 - val_loss: 0.6675 - val_accuracy: 0.8078\n",
      "Epoch 5/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0117 - accuracy: 0.9974 - val_loss: 0.6699 - val_accuracy: 0.8366\n",
      "Epoch 6/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 0.6818 - val_accuracy: 0.8379\n",
      "Epoch 7/30\n",
      "1018/1018 [==============================] - 7s 7ms/step - loss: 0.0085 - accuracy: 0.9967 - val_loss: 0.7572 - val_accuracy: 0.8340\n",
      "Epoch 00007: early stopping\n",
      "319/319 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       74        9      15\n",
      "Positif        8       94      17\n",
      "Netral         8       20      74\n",
      "{'Negatif': 74, 'Positif': 94, 'Netral': 74}\n",
      "{'Negatif': 16, 'Positif': 29, 'Netral': 32}\n",
      "{'Negatif': 24, 'Positif': 25, 'Netral': 28}\n",
      "{'Positif': 171, 'Negatif': 205, 'Netral': 185}\n",
      "akurasi :  [0.7210031347962382, 0.7586206896551724]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 37)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 37, 300)           960000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 37, 256)           384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 18, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,934,595\n",
      "Trainable params: 1,934,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1019/1019 [==============================] - 8s 7ms/step - loss: 0.6007 - accuracy: 0.6886 - val_loss: 0.4811 - val_accuracy: 0.7817\n",
      "Epoch 2/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.2881 - accuracy: 0.8822 - val_loss: 0.4099 - val_accuracy: 0.8157\n",
      "Epoch 3/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0677 - accuracy: 0.9771 - val_loss: 0.6390 - val_accuracy: 0.8261\n",
      "Epoch 4/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0246 - accuracy: 0.9921 - val_loss: 0.6949 - val_accuracy: 0.8144\n",
      "Epoch 5/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0154 - accuracy: 0.9971 - val_loss: 0.7719 - val_accuracy: 0.8157\n",
      "Epoch 6/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.8253 - val_accuracy: 0.8170\n",
      "Epoch 7/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.8185 - val_accuracy: 0.8131\n",
      "Epoch 00007: early stopping\n",
      "318/318 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       78       14       5\n",
      "Positif       10       83      26\n",
      "Netral        12       17      73\n",
      "{'Negatif': 78, 'Positif': 83, 'Netral': 73}\n",
      "{'Negatif': 22, 'Positif': 31, 'Netral': 31}\n",
      "{'Negatif': 19, 'Positif': 36, 'Netral': 29}\n",
      "{'Positif': 168, 'Negatif': 199, 'Netral': 185}\n",
      "akurasi :  [0.7210031347962382, 0.7586206896551724, 0.7358490566037735]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 36, 300)           959100    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 36, 256)           384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 18, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,933,695\n",
      "Trainable params: 1,933,695\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.5982 - accuracy: 0.6762 - val_loss: 0.4842 - val_accuracy: 0.7556\n",
      "Epoch 2/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.2999 - accuracy: 0.8747 - val_loss: 0.4337 - val_accuracy: 0.8248\n",
      "Epoch 3/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0962 - accuracy: 0.9725 - val_loss: 0.5765 - val_accuracy: 0.8235\n",
      "Epoch 4/30\n",
      "1019/1019 [==============================] - 8s 8ms/step - loss: 0.0434 - accuracy: 0.9869 - val_loss: 0.6797 - val_accuracy: 0.8196\n",
      "Epoch 5/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0154 - accuracy: 0.9941 - val_loss: 0.7632 - val_accuracy: 0.8170\n",
      "Epoch 6/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0103 - accuracy: 0.9957 - val_loss: 0.8335 - val_accuracy: 0.8183\n",
      "Epoch 7/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.8161 - val_accuracy: 0.8183\n",
      "Epoch 00007: early stopping\n",
      "318/318 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       78       15       4\n",
      "Positif        8       77      34\n",
      "Netral         6       14      82\n",
      "{'Negatif': 78, 'Positif': 77, 'Netral': 82}\n",
      "{'Negatif': 14, 'Positif': 29, 'Netral': 38}\n",
      "{'Negatif': 19, 'Positif': 42, 'Netral': 20}\n",
      "{'Positif': 170, 'Negatif': 207, 'Netral': 178}\n",
      "akurasi :  [0.7210031347962382, 0.7586206896551724, 0.7358490566037735, 0.7452830188679245]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 37)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 37, 300)           930900    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 37, 256)           384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 18, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,905,495\n",
      "Trainable params: 1,905,495\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 255 samples\n",
      "Epoch 1/30\n",
      "1019/1019 [==============================] - 8s 8ms/step - loss: 0.6080 - accuracy: 0.6804 - val_loss: 0.5602 - val_accuracy: 0.7346\n",
      "Epoch 2/30\n",
      "1019/1019 [==============================] - 8s 7ms/step - loss: 0.3509 - accuracy: 0.8499 - val_loss: 0.4019 - val_accuracy: 0.8327\n",
      "Epoch 3/30\n",
      "1019/1019 [==============================] - 7s 7ms/step - loss: 0.1215 - accuracy: 0.9581 - val_loss: 0.4738 - val_accuracy: 0.8209\n",
      "Epoch 4/30\n",
      "1019/1019 [==============================] - 8s 8ms/step - loss: 0.0419 - accuracy: 0.9912 - val_loss: 0.5786 - val_accuracy: 0.8366\n",
      "Epoch 5/30\n",
      "1019/1019 [==============================] - 8s 7ms/step - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.5777 - val_accuracy: 0.8248\n",
      "Epoch 6/30\n",
      "1019/1019 [==============================] - 8s 8ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.6751 - val_accuracy: 0.8327\n",
      "Epoch 7/30\n",
      "1019/1019 [==============================] - 8s 7ms/step - loss: 0.0113 - accuracy: 0.9961 - val_loss: 0.6803 - val_accuracy: 0.8314\n",
      "Epoch 00007: early stopping\n",
      "318/318 [==============================] - 1s 3ms/step\n",
      "         Negatif  Positif  Netral\n",
      "Negatif       75        7      15\n",
      "Positif       10       91      18\n",
      "Netral         8       23      71\n",
      "{'Negatif': 75, 'Positif': 91, 'Netral': 71}\n",
      "{'Negatif': 18, 'Positif': 30, 'Netral': 33}\n",
      "{'Negatif': 22, 'Positif': 28, 'Netral': 31}\n",
      "{'Positif': 169, 'Negatif': 203, 'Netral': 183}\n",
      "akurasi :  [0.7210031347962382, 0.7586206896551724, 0.7358490566037735, 0.7452830188679245, 0.7452830188679245]\n",
      "{'acc': [0.7210031347962382, 0.7586206896551724, 0.7358490566037735, 0.7452830188679245, 0.7452830188679245], 'presisi': [0.7472277453176659, 0.7615210240152717, 0.7366644174538912, 0.7525248382098259, 0.7470700120993378], 'recall': [0.7155528878217954, 0.7568360677604375, 0.7390963258155496, 0.7517013678323563, 0.74466006333805]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import nltk\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "data = pd.read_csv('D:/Tugas/Skripsi/tambah data neg hasil.csv' , encoding='unicode_escape',header = None)\n",
    "\n",
    "data.columns = ['Id','Text_Final','Kategori']\n",
    "\n",
    "data = data.reset_index()\n",
    "\n",
    "data['tokens'] = [word_tokenize(sen) for sen in data['Text_Final']]\n",
    "\n",
    "def removeDoublecharWordList(word_list):\n",
    "    return [removeDoublechar(word) for word in word_list]\n",
    "\n",
    "def removeDoublechar(word):\n",
    "    newWord = list(word)\n",
    "    charsList = enumerate(list(word))\n",
    "    for i,c in charsList:\n",
    "        if i > len(newWord) -2:\n",
    "            break\n",
    "        if newWord[i+1] == c:\n",
    "            newWord[i+1] = ''\n",
    "    return ''.join(newWord)\n",
    "\n",
    "dataToken = data.tokens.apply(removeDoublecharWordList).reset_index()['tokens']\n",
    "data['Text_Final'] = [' '.join(sen) for sen in dataToken]\n",
    "x=data['Text_Final']\n",
    "y=data['Kategori']\n",
    "# print(type(xc))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle = False)\n",
    "\n",
    "acc=[]\n",
    "presisi=[]\n",
    "recal=[]\n",
    "fold = 1\n",
    "kelas0_tpr = []\n",
    "kelas0_auc = []\n",
    "kelas1_tpr = []\n",
    "kelas1_auc = []\n",
    "kelas2_tpr = []\n",
    "kelas2_auc = []\n",
    "kelas3_tpr = []\n",
    "kelas3_auc = []\n",
    "micro_tpr=[]\n",
    "micro_auc=[]\n",
    "all_fpr0=[]\n",
    "all_tpr0=[]\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "   \n",
    "for train, test in kfold.split(x, y): \n",
    "    labelss= pd.DataFrame(columns=['Negatif','Positif','Netral'])\n",
    "    neg = []\n",
    "    pos = []\n",
    "    net = []\n",
    "#     print(test)\n",
    "\n",
    "    for l in y[train]:\n",
    "        if l == 'Negatif':\n",
    "            neg.append(1)\n",
    "            pos.append(0)\n",
    "            net.append(0)\n",
    "        if l == 'Positif':\n",
    "            neg.append(0)\n",
    "            pos.append(1)\n",
    "            net.append(0)\n",
    "        if l == 'Netral':\n",
    "            neg.append(0)\n",
    "            pos.append(0)\n",
    "            net.append(1)\n",
    "\n",
    "    labelss['Negatif']= neg\n",
    "    labelss['Positif']= pos\n",
    "    labelss['Netral']= net\n",
    "    \n",
    "    x[train]=x[train].astype(str)\n",
    "    x[test]=x[test].astype(str)\n",
    "\n",
    "    tok = [word_tokenize(sen) for sen in x[train]]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x[train].tolist())\n",
    "    train_word_index = tokenizer.word_index\n",
    "    training_sequences = tokenizer.texts_to_sequences(x[train].tolist())\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = max(len(x) for x in training_sequences)\n",
    "    EMBEDDING_DIM = 300\n",
    "    \n",
    "    model = Word2Vec(tok, size=300, min_count=1, window=5,sg=0)\n",
    "\n",
    "    embeddings_index = {}\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        word = list(model.wv.vocab)[i]\n",
    "        coefs = model[word]\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    \n",
    "    train_lstm_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    y_train = labelss.values\n",
    "    x_train = train_lstm_data\n",
    "\n",
    "    es_callback =EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    \n",
    "     # Kim Yoon CNN\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Concatenate\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM,  weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "\n",
    "    xx = Conv1D(256, 5, activation='relu',padding='same')(embedded_sequences)\n",
    "    xx = MaxPooling1D()(xx)\n",
    "\n",
    "    xx = Flatten()(xx)\n",
    "    xx = Dense(128, activation='relu')(xx)\n",
    "    xx= Dropout(0.3)(xx)\n",
    "    output = Dense(3, activation='softmax')(xx)\n",
    "\n",
    "    model = Model(sequence_input, output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=30,\n",
    "                        verbose=True,\n",
    "                        validation_split=(0.2),\n",
    "                        batch_size=16,shuffle = True,\n",
    "                        callbacks=[es_callback])\n",
    "\n",
    "    \n",
    "    test_sequences = tokenizer.texts_to_sequences(x[test].tolist())\n",
    "    test_lstm_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    \n",
    "    predictions = model.predict(test_lstm_data, batch_size=1, verbose=1)\n",
    "    labels= ['Negatif','Positif','Netral']\n",
    "\n",
    "    prediction_labels=[]\n",
    "    for p in predictions:\n",
    "        prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "#     xx=sum(y[test]==prediction_labels)/len(prediction_labels)\n",
    "\n",
    "    y_test= y[test].values\n",
    "    ulsan_fold = []\n",
    "    label_fold = []\n",
    "    y_tes=[]\n",
    "    for i in range(0, len(y_test)):\n",
    "        y_tes.append(y_test[i])\n",
    "\n",
    "    y_pred=[]\n",
    "    for p in predictions:\n",
    "        y_pred.append(labels[np.argmax(p)])\n",
    "    cf = []\n",
    "    cf_kategori = pd.DataFrame(\n",
    "    data=confusion_matrix(y_tes, y_pred, labels=labels),\n",
    "    columns=labels,\n",
    "    index=labels\n",
    "    )\n",
    "    cf.append(cf_kategori)\n",
    "    print(cf_kategori)\n",
    "\n",
    "    tps_kategori = {}\n",
    "    fps_kategori  = {}\n",
    "    fns_kategori  = {}\n",
    "    tns_kategori  = {}\n",
    "    for label in labels:\n",
    "        tps_kategori[label] = cf_kategori.loc[label, label]\n",
    "        fps_kategori[label] = cf_kategori[label].sum() - tps_kategori[label]\n",
    "        fns_kategori[label] = cf_kategori.loc[label].sum() - tps_kategori[label]\n",
    "\n",
    "    for label in set(y_tes):\n",
    "        tns_kategori[label] = len(y_tes) - (tps_kategori[label] + fps_kategori[label] + fns_kategori[label])\n",
    "\n",
    "    print(tps_kategori)\n",
    "    print(fps_kategori)\n",
    "    print(fns_kategori)\n",
    "    print(tns_kategori)\n",
    "    accuracyKategori=sum(tps_kategori.values())/len(y_tes)\n",
    "    acc.append(accuracyKategori)\n",
    "\n",
    "\n",
    "    tpfp_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fps_kategori.values()))]\n",
    "    precision=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfp_kategori)]\n",
    "    precisionKategori=sum(precision)/3\n",
    "    presisi.append(precisionKategori)\n",
    "\n",
    "    tpfn_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fns_kategori.values()))]\n",
    "    recall=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfn_kategori)]\n",
    "    recallKategori=sum(recall)/3\n",
    "    recal.append(recallKategori)\n",
    "    print('akurasi : ',acc) \n",
    "    \n",
    "#     model_json = model.to_json()\n",
    "#     with open('D:\\Tugas\\Skripsi\\model\\model_cnn3_kim_'+str(fold)+'.json','w') as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     model.save_weights('D:\\Tugas\\Skripsi\\model\\model_cnn3_kim_'+str(fold)+'.h5')\n",
    "#     print(\"Model saved to disk\")\n",
    "#     with open('D:/Tugas/Skripsi/model/tokenizer_model_cnn3_kim_'+str(fold)+'.pickle', 'wb') as handle:\n",
    "#         pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     pd.DataFrame.from_dict(x[test]).to_csv(\"{}.csv\".format('dataUji3fold'+str(fold)))\n",
    "#     fold=fold+1\n",
    "hasil={'acc':acc,'presisi':presisi,'recall':recal}\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(cf_kategori)\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1592/1592 [==============================] - 24s 15ms/step\n",
      "262\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 36)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 36, 100)      259000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 36, 256)      77056       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 36, 256)      102656      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 36, 256)      128256      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 18, 256)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 18, 256)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 18, 256)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 18, 768)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 13824)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          1769600     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            387         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,336,955\n",
      "Trainable params: 2,336,955\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 941 samples, validate on 105 samples\n",
      "Epoch 1/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.6288 - accuracy: 0.6656 - val_loss: 0.6304 - val_accuracy: 0.6603\n",
      "Epoch 2/20\n",
      "941/941 [==============================] - 5s 5ms/step - loss: 0.5783 - accuracy: 0.6936 - val_loss: 0.5716 - val_accuracy: 0.7143\n",
      "Epoch 3/20\n",
      "941/941 [==============================] - 5s 6ms/step - loss: 0.3073 - accuracy: 0.8721 - val_loss: 0.5741 - val_accuracy: 0.8095\n",
      "Epoch 4/20\n",
      "941/941 [==============================] - 5s 5ms/step - loss: 0.1228 - accuracy: 0.9557 - val_loss: 0.5320 - val_accuracy: 0.8127\n",
      "Epoch 5/20\n",
      "941/941 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.98 - 5s 5ms/step - loss: 0.0386 - accuracy: 0.9855 - val_loss: 0.6191 - val_accuracy: 0.8286\n",
      "Epoch 6/20\n",
      "941/941 [==============================] - 5s 6ms/step - loss: 0.0224 - accuracy: 0.9950 - val_loss: 0.8666 - val_accuracy: 0.8159\n",
      "Epoch 7/20\n",
      "941/941 [==============================] - 5s 5ms/step - loss: 0.0084 - accuracy: 0.9989 - val_loss: 0.7114 - val_accuracy: 0.8413\n",
      "Epoch 8/20\n",
      "941/941 [==============================] - 5s 6ms/step - loss: 0.0108 - accuracy: 0.9957 - val_loss: 0.8269 - val_accuracy: 0.8190\n",
      "Epoch 9/20\n",
      "941/941 [==============================] - 5s 6ms/step - loss: 0.0092 - accuracy: 0.9979 - val_loss: 0.7905 - val_accuracy: 0.8317\n",
      "Epoch 00009: early stopping\n",
      "262/262 [==============================] - 1s 4ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       68       17      22\n",
      "Negatif       11       63      15\n",
      "Netral        21        8      37\n",
      "{'Positif': 68, 'Negatif': 63, 'Netral': 37}\n",
      "{'Positif': 32, 'Negatif': 25, 'Netral': 37}\n",
      "{'Positif': 39, 'Negatif': 26, 'Netral': 29}\n",
      "{'Positif': 123, 'Negatif': 148, 'Netral': 159}\n",
      "akurasi :  [0.6412213740458015]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 37, 100)      265900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 37, 256)      77056       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 37, 256)      102656      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 37, 256)      128256      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 18, 256)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 18, 256)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 18, 256)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 18, 768)      0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 13824)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          1769600     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            387         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,343,855\n",
      "Trainable params: 2,343,855\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 941 samples, validate on 105 samples\n",
      "Epoch 1/20\n",
      "941/941 [==============================] - 6s 7ms/step - loss: 0.6321 - accuracy: 0.6628 - val_loss: 0.6173 - val_accuracy: 0.6603\n",
      "Epoch 2/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.5895 - accuracy: 0.6954 - val_loss: 0.5883 - val_accuracy: 0.6794\n",
      "Epoch 3/20\n",
      "941/941 [==============================] - 5s 6ms/step - loss: 0.3616 - accuracy: 0.8321 - val_loss: 0.4674 - val_accuracy: 0.8000\n",
      "Epoch 4/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.1706 - accuracy: 0.9426 - val_loss: 0.4483 - val_accuracy: 0.8127\n",
      "Epoch 5/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0731 - accuracy: 0.9738 - val_loss: 0.5296 - val_accuracy: 0.8095\n",
      "Epoch 6/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0490 - accuracy: 0.9812 - val_loss: 0.6764 - val_accuracy: 0.8317\n",
      "Epoch 7/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0286 - accuracy: 0.9897 - val_loss: 0.5542 - val_accuracy: 0.8127\n",
      "Epoch 8/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.6374 - val_accuracy: 0.8317\n",
      "Epoch 9/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.7934 - val_accuracy: 0.8349\n",
      "Epoch 00009: early stopping\n",
      "262/262 [==============================] - 1s 4ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       94        5       9\n",
      "Negatif       16       69       4\n",
      "Netral        25        5      35\n",
      "{'Positif': 94, 'Negatif': 69, 'Netral': 35}\n",
      "{'Positif': 41, 'Negatif': 10, 'Netral': 13}\n",
      "{'Positif': 14, 'Negatif': 20, 'Netral': 30}\n",
      "{'Positif': 113, 'Netral': 184, 'Negatif': 163}\n",
      "akurasi :  [0.6412213740458015, 0.7557251908396947]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 37, 100)      264000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 37, 256)      77056       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 37, 256)      102656      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 37, 256)      128256      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 18, 256)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 18, 256)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 18, 256)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 18, 768)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 13824)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          1769600     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            387         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,341,955\n",
      "Trainable params: 2,341,955\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 941 samples, validate on 105 samples\n",
      "Epoch 1/20\n",
      "941/941 [==============================] - 6s 7ms/step - loss: 0.6330 - accuracy: 0.6681 - val_loss: 0.6184 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.5919 - accuracy: 0.6851 - val_loss: 0.6086 - val_accuracy: 0.6571\n",
      "Epoch 3/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.3764 - accuracy: 0.8342 - val_loss: 0.4644 - val_accuracy: 0.7873\n",
      "Epoch 4/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.1407 - accuracy: 0.9465 - val_loss: 0.5237 - val_accuracy: 0.8032\n",
      "Epoch 5/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0552 - accuracy: 0.9841 - val_loss: 0.5708 - val_accuracy: 0.8159\n",
      "Epoch 6/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.7326 - val_accuracy: 0.8317\n",
      "Epoch 7/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.6033 - val_accuracy: 0.8159\n",
      "Epoch 8/20\n",
      "941/941 [==============================] - 6s 6ms/step - loss: 0.0119 - accuracy: 0.9968 - val_loss: 0.5751 - val_accuracy: 0.8254\n",
      "Epoch 00008: early stopping\n",
      "262/262 [==============================] - 1s 4ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       83       16       9\n",
      "Negatif       15       67       7\n",
      "Netral        19       10      36\n",
      "{'Positif': 83, 'Negatif': 67, 'Netral': 36}\n",
      "{'Positif': 34, 'Negatif': 26, 'Netral': 16}\n",
      "{'Positif': 25, 'Negatif': 22, 'Netral': 29}\n",
      "{'Positif': 120, 'Netral': 181, 'Negatif': 147}\n",
      "akurasi :  [0.6412213740458015, 0.7557251908396947, 0.7099236641221374]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 37, 100)      265500      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 37, 256)      77056       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 37, 256)      102656      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 37, 256)      128256      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 18, 256)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 18, 256)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 18, 256)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 18, 768)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "                                                                 max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 13824)        0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          1769600     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            387         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,343,455\n",
      "Trainable params: 2,343,455\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 942 samples, validate on 105 samples\n",
      "Epoch 1/20\n",
      "942/942 [==============================] - 6s 7ms/step - loss: 0.6309 - accuracy: 0.6621 - val_loss: 0.6155 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.5928 - accuracy: 0.6773 - val_loss: 0.5775 - val_accuracy: 0.6857\n",
      "Epoch 3/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.3693 - accuracy: 0.8241 - val_loss: 0.4671 - val_accuracy: 0.8159\n",
      "Epoch 4/20\n",
      "942/942 [==============================] - 7s 7ms/step - loss: 0.1166 - accuracy: 0.9522 - val_loss: 0.5059 - val_accuracy: 0.8159\n",
      "Epoch 5/20\n",
      "942/942 [==============================] - 7s 7ms/step - loss: 0.0445 - accuracy: 0.9869 - val_loss: 0.6564 - val_accuracy: 0.8190\n",
      "Epoch 6/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.0244 - accuracy: 0.9894 - val_loss: 0.6107 - val_accuracy: 0.8444\n",
      "Epoch 7/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.0190 - accuracy: 0.9950 - val_loss: 0.7342 - val_accuracy: 0.8444\n",
      "Epoch 8/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.0132 - accuracy: 0.9979 - val_loss: 0.6061 - val_accuracy: 0.8349\n",
      "Epoch 00008: early stopping\n",
      "261/261 [==============================] - 1s 4ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       82       11      15\n",
      "Negatif       15       66       7\n",
      "Netral        17        9      39\n",
      "{'Positif': 82, 'Negatif': 66, 'Netral': 39}\n",
      "{'Positif': 32, 'Negatif': 20, 'Netral': 22}\n",
      "{'Positif': 26, 'Negatif': 22, 'Netral': 26}\n",
      "{'Positif': 121, 'Netral': 174, 'Negatif': 153}\n",
      "akurasi :  [0.6412213740458015, 0.7557251908396947, 0.7099236641221374, 0.7164750957854407]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:182: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 37, 100)      264500      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 37, 256)      77056       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 37, 256)      102656      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 37, 256)      128256      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 18, 256)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 18, 256)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 18, 256)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 18, 768)      0           max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "                                                                 max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 13824)        0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          1769600     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            387         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,342,455\n",
      "Trainable params: 2,342,455\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fak\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 942 samples, validate on 105 samples\n",
      "Epoch 1/20\n",
      "942/942 [==============================] - 6s 7ms/step - loss: 0.6266 - accuracy: 0.6631 - val_loss: 0.6212 - val_accuracy: 0.6667\n",
      "Epoch 2/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.6020 - accuracy: 0.6808 - val_loss: 0.5975 - val_accuracy: 0.6921\n",
      "Epoch 3/20\n",
      "942/942 [==============================] - 6s 7ms/step - loss: 0.4311 - accuracy: 0.8001 - val_loss: 0.5359 - val_accuracy: 0.7651\n",
      "Epoch 4/20\n",
      "942/942 [==============================] - 7s 7ms/step - loss: 0.1618 - accuracy: 0.9462 - val_loss: 0.5414 - val_accuracy: 0.7714\n",
      "Epoch 5/20\n",
      "942/942 [==============================] - 7s 7ms/step - loss: 0.0679 - accuracy: 0.9781 - val_loss: 0.6750 - val_accuracy: 0.7968\n",
      "Epoch 6/20\n",
      "942/942 [==============================] - 6s 7ms/step - loss: 0.0385 - accuracy: 0.9876 - val_loss: 0.6135 - val_accuracy: 0.8286\n",
      "Epoch 7/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.0236 - accuracy: 0.9961 - val_loss: 0.8043 - val_accuracy: 0.7778\n",
      "Epoch 8/20\n",
      "942/942 [==============================] - 6s 6ms/step - loss: 0.0182 - accuracy: 0.9968 - val_loss: 0.7061 - val_accuracy: 0.8095\n",
      "Epoch 00008: early stopping\n",
      "261/261 [==============================] - 2s 6ms/step\n",
      "         Positif  Negatif  Netral\n",
      "Positif       75       14      18\n",
      "Negatif        9       76       4\n",
      "Netral        16        3      46\n",
      "{'Positif': 75, 'Negatif': 76, 'Netral': 46}\n",
      "{'Positif': 25, 'Negatif': 17, 'Netral': 22}\n",
      "{'Positif': 32, 'Negatif': 13, 'Netral': 19}\n",
      "{'Positif': 129, 'Netral': 174, 'Negatif': 155}\n",
      "akurasi :  [0.6412213740458015, 0.7557251908396947, 0.7099236641221374, 0.7164750957854407, 0.7547892720306514]\n",
      "{'acc': [0.6412213740458015, 0.7557251908396947, 0.7099236641221374, 0.7164750957854407, 0.7547892720306514], 'presisi': [0.631969696969697, 0.7662935614939834, 0.7073798364120943, 0.7086947894580778, 0.7478916297701877], 'recall': [0.6346617492789917, 0.7280376025694378, 0.6917245537095725, 0.7030864197530864, 0.7541864904670743]}\n",
      "71.56269193647451\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import nltk\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from statistics import mean \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "json_file_spam = open('D:\\Tugas\\Skripsi\\model\\model_spam4_kim.json', 'r')\n",
    "model_json_spam = json_file_spam.read()\n",
    "json_file_spam.close()\n",
    "model_spam = model_from_json(model_json_spam)\n",
    "model_spam.load_weights('D:\\Tugas\\Skripsi\\model\\model_spam4_kim.h5')\n",
    "\n",
    "with open('D:/Tugas/Skripsi/model/tokenizer_model_spam4_kim.pickle', 'rb') as handle:\n",
    "    tokenizer_spam = pickle.load(handle)\n",
    "\n",
    "data = pd.read_csv('D:/Tugas/Skripsi/tambah data neg hasil.csv' , encoding='unicode_escape',header = None)\n",
    "\n",
    "data.columns = ['Id','Text_Final','Kategori']\n",
    "\n",
    "data = data.reset_index()\n",
    "\n",
    "data['tokens'] = [word_tokenize(sen) for sen in data['Text_Final']]\n",
    "\n",
    "def removeDoublecharWordList(word_list):\n",
    "    return [removeDoublechar(word) for word in word_list]\n",
    "\n",
    "def removeDoublechar(word):\n",
    "    newWord = list(word)\n",
    "    charsList = enumerate(list(word))\n",
    "    for i,c in charsList:\n",
    "        if i > len(newWord) -2:\n",
    "            break\n",
    "        if newWord[i+1] == c:\n",
    "            newWord[i+1] = ''\n",
    "    return ''.join(newWord)\n",
    "\n",
    "dataToken = data.tokens.apply(removeDoublecharWordList).reset_index()['tokens']\n",
    "data['Text_Final'] = [' '.join(sen) for sen in dataToken]\n",
    "xc=data['Text_Final']\n",
    "yc=data['Kategori']\n",
    "\n",
    "        \n",
    "test_sequences = tokenizer_spam.texts_to_sequences(xc.tolist())\n",
    "test_data = pad_sequences(test_sequences, maxlen=37)\n",
    "\n",
    "xs = []\n",
    "lab_sen = []\n",
    "ul_sen = []\n",
    "prediksi_spam=[]\n",
    "predik_spam=[]\n",
    "ys = []\n",
    "yt = []\n",
    "ulasan_spam = []\n",
    "result_spam = model_spam.predict(test_data, batch_size=1, verbose=1)\n",
    "class_spam = ['SPAM','BUKAN SPAM']\n",
    "# print(type(xc))\n",
    "\n",
    "for i in range(len(result_spam)):\n",
    "    predik_spam.append(class_spam[result_spam[i].argmax()])\n",
    "    if predik_spam[i] == 'BUKAN SPAM':\n",
    "        lab_sen.append(yc[i])\n",
    "        ul_sen.append(xc[i])\n",
    "    else:\n",
    "        prediksi_spam.append('Netral')\n",
    "        ulasan_spam.append(xc[i])\n",
    "\n",
    "dat_ul = []\n",
    "y = pd.Series(lab_sen)\n",
    "x = pd.Series(ul_sen)\n",
    "\n",
    "print(len(y_tes))\n",
    "print(len(dat_ul))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "acc=[]\n",
    "presisi=[]\n",
    "recal=[]\n",
    "fold = 1\n",
    "kelas0_tpr = []\n",
    "kelas0_auc = []\n",
    "kelas1_tpr = []\n",
    "kelas1_auc = []\n",
    "kelas2_tpr = []\n",
    "kelas2_auc = []\n",
    "kelas3_tpr = []\n",
    "kelas3_auc = []\n",
    "micro_tpr=[]\n",
    "micro_auc=[]\n",
    "all_fpr0=[]\n",
    "all_tpr0=[]\n",
    "# x = data['Text_Final']\n",
    "# y = data['Kategori']\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "    \n",
    "\n",
    "for train, test in kfold.split(x, y): \n",
    "    labelss= pd.DataFrame(columns=['Negatif','Positif','Netral'])\n",
    "    neg = []\n",
    "    pos = []\n",
    "    net = []\n",
    "\n",
    "    for l in y[train]:\n",
    "        if l == 'Negatif':\n",
    "            neg.append(0)\n",
    "            pos.append(1)\n",
    "            net.append(0)\n",
    "        if l == 'Positif':\n",
    "            neg.append(1)\n",
    "            pos.append(0)\n",
    "            net.append(0)\n",
    "        if l == 'Netral':\n",
    "            neg.append(0)\n",
    "            pos.append(0)\n",
    "            net.append(1)\n",
    "\n",
    "    labelss['Positif']= pos\n",
    "    labelss['Negatif']= neg\n",
    "    labelss['Netral']= net\n",
    "    \n",
    "    x[train]=x[train].astype(str)\n",
    "    x[test]=x[test].astype(str)\n",
    "    \n",
    "    tok = [word_tokenize(sen) for sen in x[train]]\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x[train].tolist())\n",
    "    train_word_index = tokenizer.word_index\n",
    "    training_sequences = tokenizer.texts_to_sequences(x[train].tolist())\n",
    "    \n",
    "    MAX_SEQUENCE_LENGTH = max(len(x) for x in training_sequences)\n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    model_word = Word2Vec(tok, size=100, min_count=5, window=5,sg=1)\n",
    "    embeddings_index = {}\n",
    "    for i in range(len(model_word.wv.vocab)):\n",
    "        word = list(model_word.wv.vocab)[i]\n",
    "        coefs = model_word[word]\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index)+1, 100))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    \n",
    "    train_lstm_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "    y_train = labelss.values\n",
    "    x_train = train_lstm_data\n",
    "    \n",
    "#     print(len(x_train))\n",
    "\n",
    "    es_callback =EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    \n",
    "     # Kim Yoon CNN\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Concatenate\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        xx = Conv1D(256, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "        xx = MaxPooling1D()(xx)\n",
    "        convs.append(xx)\n",
    "\n",
    "    xx = Concatenate(axis=-1)(convs)\n",
    "    xx = Flatten()(xx)\n",
    "    xx = Dense(128, activation='relu')(xx)\n",
    "    xx= Dropout(0.3)(xx)\n",
    "    output = Dense(3, activation='softmax')(xx)\n",
    "\n",
    "    model = Model(sequence_input, output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=20,\n",
    "                        verbose=True,\n",
    "                        validation_split=(0.1),\n",
    "                        batch_size=16,shuffle = True,\n",
    "                        callbacks=[es_callback])\n",
    "        \n",
    "\n",
    "#     test_sequences = tokenizer_spam.texts_to_sequences(x[test].tolist())\n",
    "#     test_data = pad_sequences(test_sequences, maxlen=36)\n",
    "\n",
    "#     xs = []\n",
    "#     lab_sen = []\n",
    "#     ul_sen = []\n",
    "#     prediksi_spam=[]\n",
    "#     predik_spam=[]\n",
    "#     ys = []\n",
    "#     yt = []\n",
    "#     ulasan_spam = []\n",
    "#     result_spam = model_spam.predict(test_data, batch_size=1, verbose=1)\n",
    "#     class_spam = ['SPAM','BUKAN SPAM']\n",
    "    y_tes = []\n",
    "    y_test= y[test].values\n",
    "    for i in range(0, len(y_test)):\n",
    "        y_tes.append(y_test[i])\n",
    "\n",
    "#     ulasants = x[test].values\n",
    "#     for i in range(0, len(ulasants)):\n",
    "#         ys.append(ulasants[i])\n",
    "\n",
    "#     for i in range(len(result_spam)):\n",
    "#         predik_spam.append(class_spam[result_spam[i].argmax()])\n",
    "#         if predik_spam[i] == 'BUKAN SPAM':\n",
    "#             lab_sen.append(yt[i])\n",
    "#             ul_sen.append(ys[i])\n",
    "#         else:\n",
    "#             prediksi_spam.append('Netral')\n",
    "#             ulasan_spam.append(ys[i])\n",
    "\n",
    "#     dat_ul = []\n",
    "#     y_tes = lab_sen\n",
    "#     dat_ul = ul_sen\n",
    "    \n",
    "#     print(len(y_tes))\n",
    "#     print(len(dat_ul))\n",
    "\n",
    "\n",
    "    test_sequences = tokenizer.texts_to_sequences(x[test])\n",
    "    test_lstm_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "\n",
    "    predictions = model.predict(test_lstm_data, batch_size=1, verbose=1)\n",
    "    labels= ['Positif','Negatif','Netral']\n",
    "\n",
    "    prediction_labels=[]\n",
    "    for p in predictions:\n",
    "        prediction_labels.append(labels[np.argmax(p)])\n",
    "\n",
    "#     xx=sum(y[test]==prediction_labels)/len(prediction_labels)\n",
    "\n",
    "\n",
    "    y_pred=[]\n",
    "    for p in predictions:\n",
    "        y_pred.append(labels[np.argmax(p)])\n",
    "    cf = []\n",
    "    cf_kategori = pd.DataFrame(\n",
    "    data=confusion_matrix(y_tes, y_pred, labels=labels),\n",
    "    columns=labels,\n",
    "    index=labels\n",
    "    )\n",
    "    cf.append(cf_kategori)\n",
    "    print(cf_kategori)\n",
    "\n",
    "    tps_kategori = {}\n",
    "    fps_kategori  = {}\n",
    "    fns_kategori  = {}\n",
    "    tns_kategori  = {}\n",
    "    for label in labels:\n",
    "        tps_kategori[label] = cf_kategori.loc[label, label]\n",
    "        fps_kategori[label] = cf_kategori[label].sum() - tps_kategori[label]\n",
    "        fns_kategori[label] = cf_kategori.loc[label].sum() - tps_kategori[label]\n",
    "\n",
    "    for label in set(y_tes):\n",
    "        tns_kategori[label] = len(y_tes) - (tps_kategori[label] + fps_kategori[label] + fns_kategori[label])\n",
    "\n",
    "    print(tps_kategori)\n",
    "    print(fps_kategori)\n",
    "    print(fns_kategori)\n",
    "    print(tns_kategori)\n",
    "    accuracyKategori=sum(tps_kategori.values())/len(y_tes)\n",
    "    acc.append(accuracyKategori)\n",
    "\n",
    "\n",
    "    tpfp_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fps_kategori.values()))]\n",
    "    precision=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfp_kategori)]\n",
    "    precisionKategori=sum(precision)/3\n",
    "    presisi.append(precisionKategori)\n",
    "\n",
    "    tpfn_kategori = [ai + bi for ai, bi in zip(list(tps_kategori.values()), list(fns_kategori.values()))]\n",
    "    recall=[ai / bi  if bi>0 else 0 for ai, bi in zip(list(tps_kategori.values()), tpfn_kategori)]\n",
    "    recallKategori=sum(recall)/3\n",
    "    recal.append(recallKategori)\n",
    "    print('akurasi : ',acc)\n",
    "    \n",
    "#     model_json = model.to_json()\n",
    "#     with open('D:\\Tugas\\Skripsi\\model\\model_cnndenganspam256_kim_'+str(fold)+'.json','w') as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     model.save_weights('D:\\Tugas\\Skripsi\\model\\model_cnndenganspam256_kim_'+str(fold)+'.h5')\n",
    "#     print(\"Model saved to disk\")\n",
    "#     with open('D:/Tugas/Skripsi/model/tokenizer_model_model_cnndenganspam256_kim_'+str(fold)+'.pickle', 'wb') as handle:\n",
    "#         pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     fold = fold+1\n",
    "hasil={'acc':acc,'presisi':presisi,'recall':recal}\n",
    "print(hasil)\n",
    "\n",
    "# with open ('D:/Tugas/Skripsi/testing/1/hasil_kategori_kfold10_40.json','w') as json_file:\n",
    "#     json.dump(hasil,json_file)\n",
    "\n",
    "print(mean(acc)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('C:/Users/fak/Documents/niken code/glcm_manggoleaf_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contrast_0</th>\n",
       "      <th>contrast_45</th>\n",
       "      <th>contrast_90</th>\n",
       "      <th>contrast_135</th>\n",
       "      <th>dissimilarity_0</th>\n",
       "      <th>dissimilarity_45</th>\n",
       "      <th>dissimilarity_90</th>\n",
       "      <th>dissimilarity_135</th>\n",
       "      <th>homogeneity_0</th>\n",
       "      <th>homogeneity_45</th>\n",
       "      <th>...</th>\n",
       "      <th>ASM_135</th>\n",
       "      <th>energy_0</th>\n",
       "      <th>energy_45</th>\n",
       "      <th>energy_90</th>\n",
       "      <th>energy_135</th>\n",
       "      <th>correlation_0</th>\n",
       "      <th>correlation_45</th>\n",
       "      <th>correlation_90</th>\n",
       "      <th>correlation_135</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370.147.845.062.739</td>\n",
       "      <td>1.586.384.615.384.610</td>\n",
       "      <td>12.449.841.269.841.200</td>\n",
       "      <td>13.212.417.582.417.500</td>\n",
       "      <td>1.747.790.507.364.970</td>\n",
       "      <td>3.203.296.703.296.700</td>\n",
       "      <td>27.457.580.733.442.700</td>\n",
       "      <td>31.160.439.560.439.500</td>\n",
       "      <td>0.772832</td>\n",
       "      <td>0.759127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474195</td>\n",
       "      <td>0.693107</td>\n",
       "      <td>0.688558</td>\n",
       "      <td>0.691230</td>\n",
       "      <td>0.688618</td>\n",
       "      <td>0.994357</td>\n",
       "      <td>0.975924</td>\n",
       "      <td>0.981059</td>\n",
       "      <td>0.979948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.260.665.575.559.200</td>\n",
       "      <td>26.295.675.824.175.800</td>\n",
       "      <td>2.070.019.157.088.120</td>\n",
       "      <td>21.581.840.659.340.600</td>\n",
       "      <td>21.870.158.210.583.700</td>\n",
       "      <td>4.111.483.516.483.510</td>\n",
       "      <td>34.772.304.324.028.400</td>\n",
       "      <td>38.244.505.494.505.400</td>\n",
       "      <td>0.771249</td>\n",
       "      <td>0.757439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485997</td>\n",
       "      <td>0.702098</td>\n",
       "      <td>0.697098</td>\n",
       "      <td>0.699327</td>\n",
       "      <td>0.697135</td>\n",
       "      <td>0.994113</td>\n",
       "      <td>0.973995</td>\n",
       "      <td>0.979478</td>\n",
       "      <td>0.978657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.768.194.217.130.300</td>\n",
       "      <td>26.252.395.604.395.600</td>\n",
       "      <td>2.132.659.003.831.410</td>\n",
       "      <td>22.953.439.560.439.500</td>\n",
       "      <td>2.200.654.664.484.450</td>\n",
       "      <td>4.365.879.120.879.120</td>\n",
       "      <td>37.706.622.879.036.600</td>\n",
       "      <td>4.179.835.164.835.160</td>\n",
       "      <td>0.735245</td>\n",
       "      <td>0.712306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400943</td>\n",
       "      <td>0.638878</td>\n",
       "      <td>0.632656</td>\n",
       "      <td>0.634898</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.995013</td>\n",
       "      <td>0.977024</td>\n",
       "      <td>0.981297</td>\n",
       "      <td>0.979911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.196.508.456.082.900</td>\n",
       "      <td>1.847.612.087.912.080</td>\n",
       "      <td>14.651.986.863.711</td>\n",
       "      <td>17.539.499.999.999.900</td>\n",
       "      <td>20.639.388.979.814.500</td>\n",
       "      <td>3.539.175.824.175.820</td>\n",
       "      <td>3.028.407.224.958.940</td>\n",
       "      <td>3.528.076.923.076.920</td>\n",
       "      <td>0.752072</td>\n",
       "      <td>0.738131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431902</td>\n",
       "      <td>0.662574</td>\n",
       "      <td>0.657139</td>\n",
       "      <td>0.659288</td>\n",
       "      <td>0.657193</td>\n",
       "      <td>0.993513</td>\n",
       "      <td>0.978760</td>\n",
       "      <td>0.983119</td>\n",
       "      <td>0.979837</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.055.657.392.253.130</td>\n",
       "      <td>2.514.443.956.043.950</td>\n",
       "      <td>20.354.039.408.866.900</td>\n",
       "      <td>22.363.181.318.681.300</td>\n",
       "      <td>2.415.875.613.747.950</td>\n",
       "      <td>4.427.087.912.087.910</td>\n",
       "      <td>38.130.268.199.233.600</td>\n",
       "      <td>4.320.549.450.549.440</td>\n",
       "      <td>0.726729</td>\n",
       "      <td>0.706212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391865</td>\n",
       "      <td>0.632033</td>\n",
       "      <td>0.626064</td>\n",
       "      <td>0.628542</td>\n",
       "      <td>0.625991</td>\n",
       "      <td>0.993954</td>\n",
       "      <td>0.974987</td>\n",
       "      <td>0.979712</td>\n",
       "      <td>0.977753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               contrast_0             contrast_45             contrast_90  \\\n",
       "0     370.147.845.062.739   1.586.384.615.384.610  12.449.841.269.841.200   \n",
       "1  59.260.665.575.559.200  26.295.675.824.175.800   2.070.019.157.088.120   \n",
       "2  56.768.194.217.130.300  26.252.395.604.395.600   2.132.659.003.831.410   \n",
       "3  56.196.508.456.082.900   1.847.612.087.912.080      14.651.986.863.711   \n",
       "4   6.055.657.392.253.130   2.514.443.956.043.950  20.354.039.408.866.900   \n",
       "\n",
       "             contrast_135         dissimilarity_0       dissimilarity_45  \\\n",
       "0  13.212.417.582.417.500   1.747.790.507.364.970  3.203.296.703.296.700   \n",
       "1  21.581.840.659.340.600  21.870.158.210.583.700  4.111.483.516.483.510   \n",
       "2  22.953.439.560.439.500   2.200.654.664.484.450  4.365.879.120.879.120   \n",
       "3  17.539.499.999.999.900  20.639.388.979.814.500  3.539.175.824.175.820   \n",
       "4  22.363.181.318.681.300   2.415.875.613.747.950  4.427.087.912.087.910   \n",
       "\n",
       "         dissimilarity_90       dissimilarity_135  homogeneity_0  \\\n",
       "0  27.457.580.733.442.700  31.160.439.560.439.500       0.772832   \n",
       "1  34.772.304.324.028.400  38.244.505.494.505.400       0.771249   \n",
       "2  37.706.622.879.036.600   4.179.835.164.835.160       0.735245   \n",
       "3   3.028.407.224.958.940   3.528.076.923.076.920       0.752072   \n",
       "4  38.130.268.199.233.600   4.320.549.450.549.440       0.726729   \n",
       "\n",
       "   homogeneity_45  ...   ASM_135  energy_0  energy_45  energy_90  energy_135  \\\n",
       "0        0.759127  ...  0.474195  0.693107   0.688558   0.691230    0.688618   \n",
       "1        0.757439  ...  0.485997  0.702098   0.697098   0.699327    0.697135   \n",
       "2        0.712306  ...  0.400943  0.638878   0.632656   0.634898    0.633200   \n",
       "3        0.738131  ...  0.431902  0.662574   0.657139   0.659288    0.657193   \n",
       "4        0.706212  ...  0.391865  0.632033   0.626064   0.628542    0.625991   \n",
       "\n",
       "   correlation_0  correlation_45  correlation_90  correlation_135  label  \n",
       "0       0.994357        0.975924        0.981059         0.979948      0  \n",
       "1       0.994113        0.973995        0.979478         0.978657      0  \n",
       "2       0.995013        0.977024        0.981297         0.979911      0  \n",
       "3       0.993513        0.978760        0.983119         0.979837      0  \n",
       "4       0.993954        0.974987        0.979712         0.977753      0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
